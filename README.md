# MemoryLayer

**A skeleton framework for building AI apps with persistent memory.**

MemoryLayer is a lean, modular memory stack that gives any AI application the ability to remember. Four focused packages provide everything you need: capture conversations, extract knowledge, store in SQL+vectors, and build intelligent context.

## Hello World (5 Lines)

```typescript
import { MemoryLayer } from '@memorylayer/core';

const ml = new MemoryLayer({ storage: 'sqlite://memory.db' });
await ml.extract("Project Alpha deadline is Q4 2024");
const results = await ml.search("when is the deadline?");
// Returns: "Project Alpha deadline is Q4 2024"
```

That's it. Deploy anywhere (SQLite local, Postgres cloud), swap any piece (database, LLM, embeddings), and build your own use case.

**Proven in production**: Powers both [Ghost](apps/ghost) (blog CMS) and [Handoff](apps/handoff-frontend) (conversation manager)‚Äîtwo completely different apps built on the same skeleton.

[üìñ Getting Started](docs/GETTING_STARTED.md) | [üèóÔ∏è Architecture](docs/ARCHITECTURE.md) | [üì¶ Core Package](packages/core/README.md)

---

## Why MemoryLayer?

Instead of building bespoke "memory code" in every AI app, you get a small set of well‚Äëtyped packages for:
- **Capturing** conversations and events from different sources
- **Extracting** structured memories (entities, facts, decisions)  
- **Storing** and searching in SQL + vectors
- **Assembling** context windows for LLM calls

You can use it to power personal agents, team workspaces, support tools, or any app that needs durable, searchable AI memory.

---

## What Makes MemoryLayer Different

Unlike existing memory solutions, MemoryLayer is **actually proven** with two production apps and offers features nobody else combines:

### ‚ú® Unique Features

1. **SQLite + Postgres with Same Code**
   - Dev: `storage: 'sqlite://dev.db'`
   - Prod: `storage: { postgres: {...} }`
   - No rewrites, no adapter hell

2. **100% Local-First**
   - Runs completely offline (SQLite + local embeddings)
   - No cloud dependencies required
   - **Proven**: Ghost desktop app works with zero internet

3. **Two Production Apps, Same Foundation**
   - Ghost: Blog CMS with semantic linking
   - Handoff: Team workspace with AI briefs
   - Zero shared application code

4. **Smart Conversation Chunking**
   - Handles 200K+ token conversations
   - Automatic chunking with overlap
   - Token-accurate counting (tiktoken)

5. **Custom Memory Types**
   - Extend beyond entity/fact/decision
   - Ghost: `['topic', 'reference']`
   - Handoff: `['task', 'decision']`
   - Your app: `['bug', 'feature', 'customer']`

6. **MAKER Reliability Layer** (NEW)
   - Multi-agent consensus for robust extraction
   - 3 parallel Gemini Flash-8B calls + voting
   - Graceful fallback on failures
   - 51 tests (22 unit + 7 integration + 15 stress)

7. **True Modularity**
   - Use just storage (SQL client only)
   - Add context-engine (semantic search)
   - Add extraction when ready (LLM costs)
   - Or use `@memorylayer/core` for all-in-one

### üìä Comparison

| Feature | Mem0 | Zep | LangChain | **MemoryLayer** |
|---------|------|-----|-----------|-------------|
| SQLite Support | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |
| Postgres Support | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| Local Embeddings | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |
| Modular Packages | ‚ùå | ‚ùå | Partial | ‚úÖ |
| Custom Memory Types | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |
| Large Convo Chunking | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |
| **MAKER Reliability** | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |
| Two Production Apps | ‚ùå | ‚ùå | ‚ùå | ‚úÖ |
| Self-Hostable | Partial | ‚ùå | ‚úÖ | ‚úÖ |

**Bottom line**: MemoryLayer is the only solution that's local-first, database-agnostic, AND proven with multiple production apps.

---

## MAKER Reliability Layer

**NEW**: MAKER (Multi-Agent Knowledge Extraction & Refinement) enhances memory extraction through parallel microagents, validation, and consensus voting.

### How It Works

1. **Microagents**: Launches 3 parallel Gemini Flash-8B calls with identical prompts
2. **Red-Flagging**: Validates each response (schema checks, content quality)
3. **Voting**: Selects consensus result based on decision/todo overlap
4. **Graceful Fallback**: Returns null if all agents fail

**Benefits**:
- Improved reliability through redundancy
- Error correction via consensus voting
- Minimal latency overhead (parallel execution)
- Low cost (~$0.0003 per extraction using Flash-8B)

**Configuration** (environment variables):
```bash
MAKER_ENABLED=true              # Enable/disable MAKER
MAKER_REPLICAS=3                # Number of parallel microagents
MAKER_TEMPERATURE=0.4           # LLM temperature
MAKER_MODEL=gemini-1.5-flash-8b # Model to use
```

**Test Coverage**: 51 tests (all passing ‚úì)
- 22 unit tests (validation, voting, orchestration)
- 7 integration tests (full E2E flow)
- 15 stress tests (100 sequential, 50 concurrent, failure modes)
- 7 existing suite tests

See [`packages/core/memory-extraction`](./packages/core/memory-extraction/README.md) for detailed documentation.

---

## The Problem

Once you go beyond "toy chatbot", almost every AI product needs some version of a **memory stack**:

- Ingest conversations, tickets, logs, or notes from different sources.
- Run LLMs to extract entities, facts, decisions, and relationships.
- Store that graph with embeddings so you can search it semantically.
- Assemble a compact, token‚Äëbounded context window for each new LLM call.

This shows up in lots of places:
- personal agents and copilots ("remember what I asked last week"),
- team knowledge tools and workspaces,
- incident/ops copilots,
- support and customer success tools,
- research or learning notebooks.

Today, most teams rebuild this stack themselves:
- 4‚Äì6+ weeks of engineering,
- tightly coupled to a specific provider or framework,
- and bespoke designs that are hard to reuse in the next app.

Existing solutions tend to be:
- **Provider‚Äëlocked** ‚Äì tied to a single LLM or vendor's APIs.
- **Cloud‚Äëonly** ‚Äì difficult to run locally or on your own infra.
- **Monolithic** ‚Äì you have to adopt the whole framework instead of just storage or just context.

If you want:
- a slim storage + search layer for memories,
- plus a context engine,
- plus a flexible extraction pipeline,

you usually end up writing it yourself.

---

## The Skeleton

MemoryLayer is **intentionally minimal**‚Äîa skeleton you flesh out for your needs:

| Package | Purpose | Swap It |
|---------|---------|---------|
| **chat-capture** | Normalize logs from providers | Custom parsers |
| **memory-extraction** | Extract structured knowledge | Different LLMs, custom types |
| **storage** | SQL + vector persistence | SQLite ‚Üî Postgres, local ‚Üî cloud vectors |
| **context-engine** | Semantic search + context building | Custom ranking, templates |

Mix and match. Use one package or all four. Change what you want.

**Same skeleton, different apps:**
- **Ghost** (local blog CMS): SQLite + local vectors + topic extraction  
- **Handoff** (team workspace): Postgres + Cloudflare vectors + task extraction

Both were built on the same foundation with zero code duplication.

**Key properties:**

- ‚úÖ **Deploy anywhere** ‚Äì SQLite locally or Postgres in the cloud  
- ‚úÖ **Modular** ‚Äì use what you need, swap what you don't  
- ‚úÖ **Model‚Äëagnostic** ‚Äì OpenAI, Anthropic, Gemini, or your own provider  
- ‚úÖ **Framework‚Äëagnostic** ‚Äì works with Express, Next.js, Deno, Bun, plain Node

---

## Capabilities

- **Chat capture (`packages/core/chat-capture`)**
  - Normalize conversations from different providers/log formats into a common schema.
  - Support both static exports and live/streaming ingestion.
  - Optional PII redaction at capture time (emails, file paths, IDs).

- **Memory extraction (`packages/core/memory-extraction`)**
  - LLM‚Äëbased extraction of entities, facts, decisions, and custom memory types.
  - Multiple strategies (prompt-based, structured JSON, function-calling style).
  - Pluggable providers (e.g., OpenAI, Anthropic) with profiles for different use cases.
  - Incremental/streaming extraction and conversation chunking for long histories, with token counting and robust error handling.
  - Deduplication, validation, and deterministic IDs so memories can be updated and merged over time.
  - **MAKER reliability layer**: Multi-agent consensus extraction with 3 parallel calls, red-flagging, and voting

- **Storage layer (`packages/core/storage`)**
  - `StorageClient` over SQL backends (SQLite/Supabase/Postgres) and vector backends (Cloudflare Vectorize or local vectors).
  - Workspace-scoped API and multi-tenant isolation built into every query.
  - CRUD for users, workspaces, conversations, memories, and relationships.
  - Semantic vector search (`searchMemories`) plus filtered listing APIs.
  - Result-typed operations (`Result<T, StorageError>`), migrations, and transaction support.

- **Context engine (`packages/core/context-engine`)**
  - Semantic search over memories via the Storage layer + an embedding provider.
  - Embedding cache keyed by `(query, model)` to avoid redundant embeddings.
  - Token-aware context building with configurable templates and budgets.
  - Ranking hooks that combine similarity, recency, confidence, and custom strategies.
  - Optional relationship-aware recall when you want to pull connected memories into context.

- **Local-friendly operation**
  - First-class SQLite support and a local embedding provider (e.g., Transformers.js/Xenova).
  - Example app **Ghost** wires MemoryLayer to:
    - local embeddings,
    - a desktop daemon (Electron) with voice STT/TTS,
    - and a dashboard that shows commands, actions, and the memories used (including streaming output tokens).

- **Spec-driven development with Kiro**
  - `.kiro/specs/` contains the design, requirements, and task breakdowns for storage, context-engine, memory-extraction, chat-capture, Ghost, and Handoff.
  - The public APIs, data models, and folder structure in `packages/core` mirror those specs.
  - Roughly ~80% of MemoryLayer's implementation was "vibe coded" against these specs with Kiro, then finalized with tests and manual refinement.

---

## License

MIT
